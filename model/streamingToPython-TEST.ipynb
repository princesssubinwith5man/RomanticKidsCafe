{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96401c03-f344-4db9-83e9-7fc213ab7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time \n",
    "import ipywidgets as widgets \n",
    "import IPython.display as display \n",
    "import copy\n",
    "from ipywidgets import Video, Image\n",
    "from IPython.display import display \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d917324b-72da-4cd1-9f06-661402b92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://114.70.235.37:5030/stream?src=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d1ccac-1587-4067-a572-4f279ec99866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bc8224-fa30-4a90-afc6-dd3cab7d14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "BG_COLOR = (192, 192, 192) # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af919c0-0cfc-45dd-ad9b-42c21ecf8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose(image):\n",
    "\n",
    "    with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "        # ÌïÑÏöîÏóê Îî∞Îùº ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌï¥ Ïù¥ÎØ∏ÏßÄ ÏûëÏÑ±ÏùÑ Î∂àÍ∞ÄÎä•Ìï®ÏúºÎ°ú Í∏∞Î≥∏ ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Ìè¨Ï¶à Ï£ºÏÑùÏùÑ Ïù¥ÎØ∏ÏßÄ ÏúÑÏóê Í∑∏Î¶ΩÎãàÎã§.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "        \"\"\"\n",
    "        if results.pose_landmarks != None:\n",
    "            print(type(results.pose_landmarks))\n",
    "            break\n",
    "        \"\"\"\n",
    "\n",
    "        # Î≥¥Í∏∞ Ìé∏ÌïòÍ≤å Ïù¥ÎØ∏ÏßÄÎ•º Ï¢åÏö∞ Î∞òÏ†ÑÌï©ÎãàÎã§.\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bae0f71-6a50-4066-acd7-5e7484a5ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yshong/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-5-12 torch 1.11.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12045MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 1 tie\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 685.2ms pre-process, 83.4ms inference, 47.6ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.conf = 0.6\n",
    "\n",
    "# Images\n",
    "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efa4313-4741-4a1c-8196-6c6c3bf923e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 1 tie\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 171.1ms pre-process, 2.6ms inference, 0.6ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(imgs)\n",
    "results.print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984f7621-bd9d-4027-90cc-36685f2dc83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9f68b344ce4e1d8ee5a62ae0184aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', layout=\"Layout(border='solid')\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     crop_image\u001b[38;5;241m.\u001b[39mappend(img[\u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mymin\u001b[39m\u001b[38;5;124m'\u001b[39m]):\u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mymax\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxmin\u001b[39m\u001b[38;5;124m'\u001b[39m]):\u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxmax\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m croped \u001b[38;5;129;01min\u001b[39;00m crop_image:\n\u001b[0;32m---> 36\u001b[0m     croped \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcroped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Ìè¨Ï¶à Í≤∞Í≥º Ï∂úÎ†•\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     tmpStream \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, croped)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtobytes()\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mpose\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      8\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Ìè¨Ï¶à Ï£ºÏÑùÏùÑ Ïù¥ÎØ∏ÏßÄ ÏúÑÏóê Í∑∏Î¶ΩÎãàÎã§.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/mediapipe/python/solution_base.py:334\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    328\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    330\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    331\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    332\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vid = './before/Image/1.mp4'\n",
    "vid2 = './6-2/6-2_001-C01.mp4'\n",
    "cap = cv2.VideoCapture(url)\n",
    "prev = 0\n",
    "FPS = 10\n",
    "\n",
    "wImg = widgets.Image( \n",
    "    layout = widgets.Layout(border=\"solid\") \n",
    ") \n",
    "display(wImg)\n",
    "\n",
    "if cap.isOpened(): \n",
    "    ret, img = cap.read()\n",
    "    cur = time.time() - prev\n",
    "    while ret:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # ÎèôÏòÅÏÉÅ ÌååÏùºÏóêÏÑú Ï∫°Ï≥êÎêú Ïù¥ÎØ∏ÏßÄÎ•º Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ïä§Ìä∏Î¶ºÏúºÎ°ú Îã§Ïãú Ïù∏ÏΩîÎî©ÏùÑ ÌïúÎã§. \n",
    "        results = model(img)\n",
    "        \n",
    "        # xyÏóêÎäî Ïù∏ÏãùÌïú Í∞Å Í∞ùÏ≤¥, xy Ï¢åÌëúÎ°ú Íµ¨ÏÑ±Îêú Îç∞Ïù¥ÌÑ∞ ÌîÑÎ†àÏûÑ Îã¥ÍπÄ\n",
    "        xy = results.pandas().xyxy[0]\n",
    "        \n",
    "        # personÏóêÎäî Ïù∏ÏãùÌïú Í∞ùÏ≤¥Ï§ë 'person'Îßå Ï∂îÏ∂ú\n",
    "        person = xy[xy['name'] == 'person']\n",
    "        \n",
    "        crop_image = []\n",
    "        \n",
    "        # Ïù¥ÎØ∏ÏßÄ \n",
    "        for index, row in person.iterrows():\n",
    "            # Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÏÇ¨ÎûåÏóê Ìï¥ÎãπÌïòÎäî Î∂ÄÎ∂ÑÎßå ÌÅ¨Î°≠\n",
    "            # ÏÇ¨ÏßÑ ÌïúÏû•(img) -> ÏÇ¨Îûå ÏÇ¨ÏßÑ Ïó¨Îü¨Ïû•(crop_image[])\n",
    "            crop_image.append(img[int(row['ymin']):int(row['ymax']), int(row['xmin']):int(row['xmax'])])\n",
    "\n",
    "        \n",
    "        for croped in crop_image:\n",
    "            croped = pose(croped)\n",
    "            \n",
    "            # Ìè¨Ï¶à Í≤∞Í≥º Ï∂úÎ†•\n",
    "            tmpStream = cv2.imencode(\".jpeg\", croped)[1].tobytes()\n",
    "            wImg.value = tmpStream \n",
    "\n",
    "        \n",
    "        ret, img = cap.read()\n",
    "else:\n",
    "    print(\"not opened\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887747d-c3f2-4c46-b163-1989bc6bc799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
