{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96401c03-f344-4db9-83e9-7fc213ab7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time \n",
    "import ipywidgets as widgets \n",
    "import IPython.display as display \n",
    "import copy\n",
    "from ipywidgets import Video, Image\n",
    "from IPython.display import display \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import IPython\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d917324b-72da-4cd1-9f06-661402b92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://114.70.235.37:8091/?action=stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d1ccac-1587-4067-a572-4f279ec99866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bc8224-fa30-4a90-afc6-dd3cab7d14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "BG_COLOR = (192, 192, 192) # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bae0f71-6a50-4066-acd7-5e7484a5ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stu1/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-5-22 Python-3.8.12 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12045MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 1 tie\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 644.6ms pre-process, 4.0ms inference, 0.7ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.conf = 0.6\n",
    "\n",
    "# Images\n",
    "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e58c3cc-6dca-4664-be29-4d28ac63e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as multi\n",
    "from mp_test import pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984f7621-bd9d-4027-90cc-36685f2dc83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7924151fe349ad861318e15f9c8432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', layout=\"Layout(border='solid')\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "vid = './before/Image/1.mp4'\n",
    "vid2 = './6-2/6-2_001-C01.mp4'\n",
    "cap = cv2.VideoCapture(vid2)\n",
    "wImg = widgets.Image( \n",
    "    layout = widgets.Layout(border=\"solid\") \n",
    ") \n",
    "display(wImg)\n",
    "pool = Pool(os.cpu_count())\n",
    "if cap.isOpened(): \n",
    "    ret, img = cap.read()\n",
    "    while ret:\n",
    "        st = time.time()\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # 동영상 파일에서 캡쳐된 이미지를 이미지 파일 스트림으로 다시 인코딩을 한다. \n",
    "        results = model(img)\n",
    "        \n",
    "        # xy에는 인식한 각 객체, xy 좌표로 구성된 데이터 프레임 담김\n",
    "        xy = results.pandas().xyxy[0]\n",
    "        \n",
    "        # person에는 인식한 객체중 'person'만 추출\n",
    "        person = xy[xy['name'] == 'person']\n",
    "        \n",
    "        crop_image = []\n",
    "        results_crop = []\n",
    "        # 이미지 \n",
    "        for index, row in person.iterrows():\n",
    "            # 이미지에서 사람에 해당하는 부분만 크롭\n",
    "            # 사진 한장(img) -> 사람 사진 여러장(crop_image[])\n",
    "            crop_image.append(img[int(row['ymin']):int(row['ymax']), int(row['xmin']):int(row['xmax'])])          \n",
    "            croped = pool.map(pose,crop_image)\n",
    "            \n",
    "        for res in croped:\n",
    "            tmpStream = cv2.imencode(\".jpeg\", res)[1].tobytes()\n",
    "            wImg.value = tmpStream\n",
    "        \"\"\"\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        str = \"FPS : %0.1f\" % fps        \n",
    "        cv2.putText(img, str, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255),3)\n",
    "        \"\"\"\n",
    "        ret, img = cap.read()\n",
    "else:\n",
    "    print(\"not opened\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dde300-33ab-4292-9ef0-ac10888c7752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ff4f5-e0d2-402a-902e-3ce8989d2072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
