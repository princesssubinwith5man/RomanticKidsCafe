{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53fdfac-882e-43b6-bf14-384d68e11c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad04167d-8ea1-4b7a-b1ab-057e12a6a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14597f89-8c71-4284-8590-4285f0e276f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('/project/annotation/train_v2.pt')\n",
    "test_data = torch.load('/project/annotation/test_v2.pt')\n",
    "valid_data = torch.load('/project/annotation/valid_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c016475d-7651-4747-90de-e69416404288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 32\n",
    "device='cuda'\n",
    "\n",
    "# make sure to SHUFFLE for your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328bc3b-f168-4091-ad08-54809a5beed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d58f52e-8ad2-44de-abb2-d1d7a21acbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_size, output_size, hidden_dim, n_layers):\n",
    "        \n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.lstm = nn.LSTM(seq_size, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        ## lstm_out.shape [batch, seq_len, hidden_dim]\n",
    "        ## hidden [batch, hidden_dim]\n",
    "\n",
    "        # fully connected layer        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        ## out.shape: [n_layer * n_direction, batch, output_size]\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc6e69a-b06e-401c-950a-e29c43f436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        # nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3e5842-3397-4f6a-a1b1-2a178dba1946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(20, 512, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "n_layers = 4\n",
    "lr = 0.01\n",
    "output_size = 1\n",
    "\n",
    "model = SentimentLSTM(20, output_size, hidden_dim, n_layers)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24a76cf-2d0c-4552-9e5c-6f4d6f10f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss().cuda()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f120e3df-1259-4ff3-bcfd-c8ba04fe57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [01:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(f'target shape: {target.shape}')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(f'outputs shape: {outputs.shape}')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(outputs[0])\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(target[0])\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(lstm.parameters(), 10)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "num_epochs = 7\n",
    "device = 'cuda'\n",
    "m = nn.Softmax(dim = 1)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs+1)): \n",
    "    # train\n",
    "\n",
    "    losses = []\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        inputs, target = inputs.cuda(), target.cuda()\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, hidden = model(inputs)\n",
    "\n",
    "\n",
    "        # print(f'target shape: {target.shape}')\n",
    "        # print(f'outputs shape: {outputs.shape}')\n",
    "        # print(outputs[0])\n",
    "        # print(target[0])\n",
    "        loss = criterion(outputs.squeeze(), target.float())\n",
    "        loss.backward()\n",
    "\n",
    "        # scheduler.step()\n",
    "        # torch.nn.utils.clip_grad_norm_(lstm.parameters(), 10)\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(np.mean(np.array(losses)))\n",
    "\n",
    "    losses = []\n",
    "    for i, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs, target = inputs.cuda(), target.cuda()\n",
    "\n",
    "        model.eval()\n",
    "        valid, hidden = model(inputs)\n",
    "\n",
    "        #inverse\n",
    "        # valid = torch.from_numpy(scaler.inverse_transform(valid.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "        vall_loss = criterion(valid.squeeze(), target.float())\n",
    "        # scheduler.step(vall_loss)\n",
    "        losses.append(vall_loss.item())\n",
    "\n",
    "    valid_losses.append(np.mean(np.array(losses)))\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        # print(criterion1(outputs, y_train.to(device),quantile))\n",
    "\n",
    "        print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f lr: %1.5f \" %(epoch, train_losses[-1],valid_losses[-1],\n",
    "                                                                        optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "    torch.save(model.state_dict(), './model_weight.pth')\n",
    "    # model.load_state_dict(torch.load(SAVEPATH+'model_weight.pth'))\n",
    "\n",
    "    # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "    # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "    \n",
    "    \"\"\"\n",
    "    early_stopping(round(valid_losses[-1],5), model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f lr: %1.5f \" %(epoch, train_losses[-1],valid_losses[-1],\n",
    "                                                                      optimizer.param_groups[0][\"lr\"]))\n",
    "        break\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b68a9e-b20a-4704-9095-68b744e0c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val1 = None\n",
    "tor1 = None\n",
    "for i, (input, target) in enumerate(test_loader):\n",
    "    \n",
    "    input, target = input.cuda(), target.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    valid, hidden = model(input.to(device))\n",
    "    \n",
    "\n",
    "    #inverse\n",
    "    # valid = torch.from_numpy(scaler.inverse_transform(valid.cpu().detach().numpy()))\n",
    "\n",
    "    vall_loss = criterion(valid.squeeze(), target.float())\n",
    "    val1 = valid\n",
    "    tor1 = target.type(torch.long).to(device)\n",
    "    # scheduler.step(vall_loss)\n",
    "    losses.append(vall_loss.item())\n",
    "    \n",
    "\n",
    "#valid_losses.append(np.mean(np.array(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb99fee0-58b6-4574-a172-875f3d42785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5335, 0.0797, 0.1199, 0.1005, 0.8721, 0.1345, 0.7692, 0.0942, 0.0909,\n",
      "        0.1051, 0.2811, 0.1523, 0.1962, 0.1282, 0.2091, 0.2729, 0.1951, 0.1049,\n",
      "        0.0715, 0.4655, 0.2435, 0.1490, 0.1192, 0.1649, 0.0458, 0.4375, 0.1751,\n",
      "        0.1187, 0.1278, 0.1752, 0.0872], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(val1.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e2437a5-f7b8-42bc-b4f0-2ac669830ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93d66f7a-3e32-4ee8-98a4-729b5f686139",
   "metadata": {},
   "outputs": [],
   "source": [
    "vall_loss = criterion(val1.squeeze(), tor1.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b75cd47-4d67-4738-b7a3-21ff1aa98981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5173, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(vall_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "642b5693-0596-447e-8a89-f3f549c0d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n"
     ]
    }
   ],
   "source": [
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb3e8f62-36a9-4716-9d8e-279d251bdfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31327\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b690af1-1e73-4c22-bf0a-fbf3556a60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, criterion, optimizer,\n",
    "          epochs = 4, print_every = 1000, clip=5):\n",
    "    counter = 0\n",
    "\n",
    "    # move model to GPU, if available\n",
    "\n",
    "    model.train()\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "      # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = model(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = model(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                    \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0862a49-db22-464c-9a77-178d7c0fdd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 1000... Loss: 0.713335... Val Loss: 0.665098\n",
      "Epoch: 1/4... Step: 2000... Loss: 0.665361... Val Loss: 0.667735\n",
      "Epoch: 1/4... Step: 3000... Loss: 0.697498... Val Loss: 0.680759\n",
      "Epoch: 1/4... Step: 4000... Loss: 0.680513... Val Loss: 0.668773\n",
      "Epoch: 1/4... Step: 5000... Loss: 0.647701... Val Loss: 0.657261\n",
      "Epoch: 1/4... Step: 6000... Loss: 0.644167... Val Loss: 0.657255\n",
      "Epoch: 1/4... Step: 7000... Loss: 0.622219... Val Loss: 0.663902\n",
      "Epoch: 2/4... Step: 8000... Loss: 0.679453... Val Loss: 0.661010\n",
      "Epoch: 2/4... Step: 9000... Loss: 0.582800... Val Loss: 0.676661\n",
      "Epoch: 2/4... Step: 10000... Loss: 0.775414... Val Loss: 0.687700\n",
      "Epoch: 2/4... Step: 11000... Loss: 0.627910... Val Loss: 0.657226\n",
      "Epoch: 2/4... Step: 12000... Loss: 0.676351... Val Loss: 0.666580\n",
      "Epoch: 2/4... Step: 13000... Loss: 0.667362... Val Loss: 0.705565\n",
      "Epoch: 2/4... Step: 14000... Loss: 0.632252... Val Loss: 0.657484\n",
      "Epoch: 2/4... Step: 15000... Loss: 0.530082... Val Loss: 0.682073\n",
      "Epoch: 3/4... Step: 16000... Loss: 0.589778... Val Loss: 0.677729\n",
      "Epoch: 3/4... Step: 17000... Loss: 0.643803... Val Loss: 0.666336\n",
      "Epoch: 3/4... Step: 18000... Loss: 0.644069... Val Loss: 0.657259\n",
      "Epoch: 3/4... Step: 19000... Loss: 0.708913... Val Loss: 0.683691\n",
      "Epoch: 3/4... Step: 20000... Loss: 0.688083... Val Loss: 0.693358\n",
      "Epoch: 3/4... Step: 21000... Loss: 0.701307... Val Loss: 0.658123\n",
      "Epoch: 3/4... Step: 22000... Loss: 0.678195... Val Loss: 0.668234\n",
      "Epoch: 3/4... Step: 23000... Loss: 0.814055... Val Loss: 0.657435\n",
      "Epoch: 4/4... Step: 24000... Loss: 0.661593... Val Loss: 0.659135\n",
      "Epoch: 4/4... Step: 25000... Loss: 0.718507... Val Loss: 0.657538\n",
      "Epoch: 4/4... Step: 26000... Loss: 0.671166... Val Loss: 0.659374\n",
      "Epoch: 4/4... Step: 27000... Loss: 0.594164... Val Loss: 0.669988\n",
      "Epoch: 4/4... Step: 28000... Loss: 0.715163... Val Loss: 0.658636\n",
      "Epoch: 4/4... Step: 29000... Loss: 0.700298... Val Loss: 0.661024\n",
      "Epoch: 4/4... Step: 30000... Loss: 0.667254... Val Loss: 0.660296\n",
      "Epoch: 4/4... Step: 31000... Loss: 0.626795... Val Loss: 0.660524\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4866683a-25c7-4da4-aa61-cdce47cc0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, sequence_length=30):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = 32\n",
    "    acc_list = list()\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        output, h = model(inputs)\n",
    "        \n",
    "        pred = torch.round(output.squeeze())\n",
    "        \n",
    "        accuracy = (pred==labels).sum().item() / 32\n",
    "        acc_list.append(accuracy)\n",
    "\n",
    "        #print('Prediction value, pre-rounding: ', output.item())\n",
    "\n",
    "        # print custom response based on whether test_review is pos/neg\n",
    "        \n",
    "    print(np.mean(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c119dffe-7a10-4778-87f3-d85b17b0f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943596782431052\n"
     ]
    }
   ],
   "source": [
    "predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0fb27-b66c-480a-bb39-16f0740eac18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
