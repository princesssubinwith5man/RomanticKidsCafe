{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53fdfac-882e-43b6-bf14-384d68e11c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad04167d-8ea1-4b7a-b1ab-057e12a6a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14597f89-8c71-4284-8590-4285f0e276f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('/project/annotation/train.pt')\n",
    "test_data = torch.load('/project/annotation/test.pt')\n",
    "valid_data = torch.load('/project/annotation/valid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c016475d-7651-4747-90de-e69416404288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 16\n",
    "device='cuda'\n",
    "\n",
    "# make sure to SHUFFLE for your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328bc3b-f168-4091-ad08-54809a5beed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d58f52e-8ad2-44de-abb2-d1d7a21acbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_size, output_size, hidden_dim, n_layers):\n",
    "        \n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.lstm = nn.LSTM(seq_size, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        ## lstm_out.shape [batch, seq_len, hidden_dim]\n",
    "        ## hidden [batch, hidden_dim]\n",
    "\n",
    "        # fully connected layer        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        ## out.shape: [n_layer * n_direction, batch, output_size]\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc6e69a-b06e-401c-950a-e29c43f436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        # nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3e5842-3397-4f6a-a1b1-2a178dba1946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(32, 512, num_layers=4, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "n_layers = 4\n",
    "lr = 0.01\n",
    "output_size = 1\n",
    "\n",
    "model = SentimentLSTM(32, output_size, hidden_dim, n_layers)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24a76cf-2d0c-4552-9e5c-6f4d6f10f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120e3df-1259-4ff3-bcfd-c8ba04fe57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [02:28<24:47, 148.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.10245 valid loss:  0.09295 lr: 0.01000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [07:27<19:55, 149.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss: 0.10238 valid loss:  0.10376 lr: 0.01000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [12:27<14:57, 149.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss: 0.10137 valid loss:  0.10024 lr: 0.01000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [17:27<09:58, 149.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, loss: 0.10252 valid loss:  0.09274 lr: 0.01000 \n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "num_epochs = 10\n",
    "device = 'cuda'\n",
    "m = nn.Softmax(dim = 1)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs+1)): \n",
    "    # train\n",
    "\n",
    "    losses = []\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        inputs, target = inputs.cuda(), target.cuda()\n",
    "\n",
    "        model.train()\n",
    "        outputs, hidden = model(inputs.to(device))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # print(f'target shape: {target.shape}')\n",
    "        # print(f'outputs shape: {outputs.shape}')\n",
    "        # print(outputs[0])\n",
    "        # print(target[0])\n",
    "        loss = criterion(outputs.squeeze(), target.float())\n",
    "        loss.backward()\n",
    "\n",
    "        # scheduler.step()\n",
    "        # torch.nn.utils.clip_grad_norm_(lstm.parameters(), 10)\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(np.array(losses)))\n",
    "\n",
    "    losses = []\n",
    "    for i, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs, target = inputs.cuda(), target.cuda()\n",
    "\n",
    "        model.eval()\n",
    "        valid, hidden = model(inputs.to(device))\n",
    "\n",
    "        #inverse\n",
    "        # valid = torch.from_numpy(scaler.inverse_transform(valid.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "        vall_loss = criterion(valid.squeeze(), target.float())\n",
    "        # scheduler.step(vall_loss)\n",
    "        losses.append(vall_loss.item())\n",
    "\n",
    "    valid_losses.append(np.mean(np.array(losses)))\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        # print(criterion1(outputs, y_train.to(device),quantile))\n",
    "\n",
    "        print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f lr: %1.5f \" %(epoch, train_losses[-1],valid_losses[-1],\n",
    "                                                                        optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "    torch.save(model.state_dict(), './model_weight.pth')\n",
    "    # model.load_state_dict(torch.load(SAVEPATH+'model_weight.pth'))\n",
    "\n",
    "    # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "    # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "    \n",
    "    \"\"\"\n",
    "    early_stopping(round(valid_losses[-1],5), model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f lr: %1.5f \" %(epoch, train_losses[-1],valid_losses[-1],\n",
    "                                                                      optimizer.param_groups[0][\"lr\"]))\n",
    "        break\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935460f-178f-4c01-bdca-1a6a3faafaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c947f99-5f54-4ef6-8511-e149a59b1858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.3900e+02, 9.3500e+02, 2.0000e+00,  ..., 1.1120e+03, 6.5300e+02,\n",
      "         2.0000e+00],\n",
      "        [9.3900e+02, 9.3500e+02, 2.0000e+00,  ..., 1.1120e+03, 6.5300e+02,\n",
      "         2.0000e+00],\n",
      "        [9.3900e+02, 9.3500e+02, 2.0000e+00,  ..., 1.1120e+03, 6.5300e+02,\n",
      "         2.0000e+00],\n",
      "        ...,\n",
      "        [9.3900e+02, 9.3500e+02, 2.0000e+00,  ..., 1.0520e+03, 6.6400e+02,\n",
      "         2.0000e+00],\n",
      "        [9.3900e+02, 9.3500e+02, 2.0000e+00,  ..., 1.0520e+03, 6.6400e+02,\n",
      "         1.0000e+00],\n",
      "        [9.3900e+02, 9.3500e+02, 2.0000e+00,  ..., 1.0620e+03, 6.7000e+02,\n",
      "         1.0000e+00]])\n",
      "tensor(0, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input, target = test_data[0]\n",
    "\n",
    "print(input)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9f44b-e6f2-4b9d-ad6f-6fc52c790aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbac73-da28-463e-a037-3762357ed896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b68a9e-b20a-4704-9095-68b744e0c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val1 = None\n",
    "tor1 = None\n",
    "for i, (input, target) in enumerate(test_loader):\n",
    "    \n",
    "    input, target = input.cuda(), target.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    valid, hidden = model(input.to(device))\n",
    "    \n",
    "\n",
    "    #inverse\n",
    "    # valid = torch.from_numpy(scaler.inverse_transform(valid.cpu().detach().numpy()))\n",
    "\n",
    "    vall_loss = criterion(valid.squeeze(), target.float())\n",
    "    val1 = valid\n",
    "    tor1 = target.type(torch.long).to(device)\n",
    "    # scheduler.step(vall_loss)\n",
    "    losses.append(vall_loss.item())\n",
    "    \n",
    "\n",
    "#valid_losses.append(np.mean(np.array(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb99fee0-58b6-4574-a172-875f3d42785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13],\n",
      "        [1.3292e-13]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2437a5-f7b8-42bc-b4f0-2ac669830ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d66f7a-3e32-4ee8-98a4-729b5f686139",
   "metadata": {},
   "outputs": [],
   "source": [
    "vall_loss = criterion(val1.squeeze(), tor1.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75cd47-4d67-4738-b7a3-21ff1aa98981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vall_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642b5693-0596-447e-8a89-f3f549c0d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n"
     ]
    }
   ],
   "source": [
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb3e8f62-36a9-4716-9d8e-279d251bdfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20217\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b690af1-1e73-4c22-bf0a-fbf3556a60c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
