{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67dc6a8c-a46c-419f-871e-e23f496faf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842d6336-963c-4cef-bff1-1621457f1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local', 'mydb']\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(host='114.70.235.37', port=27017)\n",
    "print(client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6062931-3d40-4767-89c6-32ae0c68f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['mydb'].xy\n",
    "result= db.find().sort([(\"date\", pymongo.DESCENDING)]).limit(2)\n",
    "\n",
    "data = list(result)[0]\n",
    "start_date = data[\"date\"] - datetime.timedelta(seconds=5)\n",
    "seq = list(db.find({\"date\": {'$gte' : start_date, '$lt' : data[\"date\"]}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6025d2c-aeef-4deb-92dc-b6f9db32eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = list()\n",
    "for j in seq:\n",
    "    del j['date']\n",
    "    del j['_id']\n",
    "    \n",
    "    list_data.append(list(j.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dde7f9a-93df-4a72-9bfa-80086c559113",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = np.array(list_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef85e5f1-6501-475f-ab54-dccfc0d096a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 20)\n",
      "(183,)\n"
     ]
    }
   ],
   "source": [
    "y_false = np.array([0 for w in range(len(list_data))])\n",
    "print(list_data.shape)\n",
    "print(y_false.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b033cd7-5962-4f6f-944e-b8727bcc2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 32\n",
    "device='cuda'\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_size, output_size, hidden_dim, n_layers):\n",
    "        \n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.lstm = nn.LSTM(seq_size, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        ## lstm_out.shape [batch, seq_len, hidden_dim]\n",
    "        ## hidden [batch, hidden_dim]\n",
    "\n",
    "        # fully connected layer        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        ## out.shape: [n_layer * n_direction, batch, output_size]\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbe1f2a-6b88-4010-afab-9cd7ebfc9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        # nn.init.uniform_(param.data, -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16414ae-a0da-400c-a821-f2ba4a663dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(20, 512, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "n_layers = 4\n",
    "lr = 0.01\n",
    "output_size = 1\n",
    "\n",
    "model = SentimentLSTM(20, output_size, hidden_dim, n_layers)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e9e89c5-ab0a-435c-8846-4786c3b236ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_weight.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e1b8cb-a16d-4e87-9f45-3ba6cb6f0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, sequence_length=30):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = 32\n",
    "    acc_list = list()\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        output, h = model(inputs)\n",
    "        \n",
    "        pred = torch.round(output.squeeze())\n",
    "        \n",
    "        accuracy = (pred==labels).sum().item() / 32\n",
    "        acc_list.append(accuracy)\n",
    "\n",
    "        #print('Prediction value, pre-rounding: ', output.item())\n",
    "\n",
    "        # print custom response based on whether test_review is pos/neg\n",
    "        \n",
    "    print(np.mean(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809ff3bd-8e4d-4a02-8bd6-95eb68d93e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = TensorDataset(torch.from_numpy(list_data), torch.from_numpy(y_false))\n",
    "tmp_loader = DataLoader(tmp_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "970975e9-cf3d-4114-b11a-f5682a7147af",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, test_loader, sequence_length)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m      9\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 11\u001b[0m     output, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(output\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     15\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m==\u001b[39mlabels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mSentimentLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m## lstm_out.shape [batch, seq_len, hidden_dim]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m## hidden [batch, hidden_dim]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# fully connected layer        \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[43mlstm_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m## out.shape: [n_layer * n_direction, batch, output_size]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# sigmoid function\u001b[39;00m\n\u001b[1;32m     34\u001b[0m sig_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msig(out)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "predict(model, tmp_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a487605-1d30-4f9c-9691-ed4fdfad4a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ -22.9623,  -37.8234, -118.5906, -235.3448,   22.9623,   37.8234,\n",
       "          -85.7970,  -33.0598,    0.0000,    0.0000,  -20.1370, -140.5871,\n",
       "           43.0994,  178.4105,   43.0994,  178.4105,    0.0000,    0.0000,\n",
       "            0.0000,    0.0000]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33e5c4-2d83-40d7-9b31-c05d5736e8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
