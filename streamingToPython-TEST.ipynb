{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96401c03-f344-4db9-83e9-7fc213ab7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time \n",
    "import ipywidgets as widgets \n",
    "import IPython.display as display \n",
    "import copy\n",
    "from ipywidgets import Video, Image\n",
    "from IPython.display import display \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d917324b-72da-4cd1-9f06-661402b92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://114.70.235.37:8091/?action=stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d1ccac-1587-4067-a572-4f279ec99866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95bc8224-fa30-4a90-afc6-dd3cab7d14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "BG_COLOR = (192, 192, 192) # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af919c0-0cfc-45dd-ad9b-42c21ecf8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose(image):\n",
    "\n",
    "    with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "        # ÌïÑÏöîÏóê Îî∞Îùº ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌï¥ Ïù¥ÎØ∏ÏßÄ ÏûëÏÑ±ÏùÑ Î∂àÍ∞ÄÎä•Ìï®ÏúºÎ°ú Í∏∞Î≥∏ ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Ìè¨Ï¶à Ï£ºÏÑùÏùÑ Ïù¥ÎØ∏ÏßÄ ÏúÑÏóê Í∑∏Î¶ΩÎãàÎã§.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "        \"\"\"\n",
    "        if results.pose_landmarks != None:\n",
    "            print(type(results.pose_landmarks))\n",
    "            break\n",
    "        \"\"\"\n",
    "\n",
    "        # Î≥¥Í∏∞ Ìé∏ÌïòÍ≤å Ïù¥ÎØ∏ÏßÄÎ•º Ï¢åÏö∞ Î∞òÏ†ÑÌï©ÎãàÎã§.\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bae0f71-6a50-4066-acd7-5e7484a5ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stu1/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-5-22 Python-3.8.12 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12045MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 1 tie\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 622.4ms pre-process, 5.5ms inference, 1.0ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.conf = 0.6\n",
    "\n",
    "# Images\n",
    "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efa4313-4741-4a1c-8196-6c6c3bf923e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 1 tie\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 154.2ms pre-process, 3.1ms inference, 1.9ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(imgs)\n",
    "results.print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984f7621-bd9d-4027-90cc-36685f2dc83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386c0d6f84cb49f7a701da2cac26be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', layout=\"Layout(border='solid')\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vid = './before/Image/1.mp4'\n",
    "vid2 = './6-2/6-2_001-C01.mp4'\n",
    "cap = cv2.VideoCapture(vid2)\n",
    "prev = 0\n",
    "FPS = 10\n",
    "\n",
    "wImg = widgets.Image( \n",
    "    layout = widgets.Layout(border=\"solid\") \n",
    ") \n",
    "display(wImg)\n",
    "\n",
    "if cap.isOpened(): \n",
    "    ret, img = cap.read()\n",
    "    cur = time.time() - prev\n",
    "    while ret:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # ÎèôÏòÅÏÉÅ ÌååÏùºÏóêÏÑú Ï∫°Ï≥êÎêú Ïù¥ÎØ∏ÏßÄÎ•º Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ïä§Ìä∏Î¶ºÏúºÎ°ú Îã§Ïãú Ïù∏ÏΩîÎî©ÏùÑ ÌïúÎã§. \n",
    "        results = model(img)\n",
    "        \n",
    "        # xyÏóêÎäî Ïù∏ÏãùÌïú Í∞Å Í∞ùÏ≤¥, xy Ï¢åÌëúÎ°ú Íµ¨ÏÑ±Îêú Îç∞Ïù¥ÌÑ∞ ÌîÑÎ†àÏûÑ Îã¥ÍπÄ\n",
    "        xy = results.pandas().xyxy[0]\n",
    "        \n",
    "        # personÏóêÎäî Ïù∏ÏãùÌïú Í∞ùÏ≤¥Ï§ë 'person'Îßå Ï∂îÏ∂ú\n",
    "        person = xy[xy['name'] == 'person']\n",
    "        \n",
    "        crop_image = []\n",
    "        \n",
    "        # Ïù¥ÎØ∏ÏßÄ \n",
    "        for index, row in person.iterrows():\n",
    "            # Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÏÇ¨ÎûåÏóê Ìï¥ÎãπÌïòÎäî Î∂ÄÎ∂ÑÎßå ÌÅ¨Î°≠\n",
    "            # ÏÇ¨ÏßÑ ÌïúÏû•(img) -> ÏÇ¨Îûå ÏÇ¨ÏßÑ Ïó¨Îü¨Ïû•(crop_image[])\n",
    "            crop_image.append(img[int(row['ymin']):int(row['ymax']), int(row['xmin']):int(row['xmax'])])\n",
    "\n",
    "        \n",
    "        for croped in crop_image:\n",
    "            croped = pose(croped)\n",
    "            \n",
    "            # Ìè¨Ï¶à Í≤∞Í≥º Ï∂úÎ†•\n",
    "            tmpStream = cv2.imencode(\".jpeg\", croped)[1].tobytes()\n",
    "            wImg.value = tmpStream \n",
    "\n",
    "        \n",
    "        ret, img = cap.read()\n",
    "else:\n",
    "    print(\"not opened\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887747d-c3f2-4c46-b163-1989bc6bc799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
