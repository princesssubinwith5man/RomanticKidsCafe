{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96401c03-f344-4db9-83e9-7fc213ab7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time \n",
    "import ipywidgets as widgets \n",
    "import IPython.display as display \n",
    "import copy\n",
    "from ipywidgets import Video, Image\n",
    "from IPython.display import display \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "import copy\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d917324b-72da-4cd1-9f06-661402b92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://114.70.235.37:8091/?action=stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1c5b9-d666-4196-8f2c-793368340430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bbf9d-2fbb-4cdc-8b0a-50140680183b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d1ccac-1587-4067-a572-4f279ec99866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95bc8224-fa30-4a90-afc6-dd3cab7d14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "BG_COLOR = (192, 192, 192) # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af919c0-0cfc-45dd-ad9b-42c21ecf8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose(image):\n",
    "\n",
    "    with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "        # 필요에 따라 성능 향상을 위해 이미지 작성을 불가능함으로 기본 설정합니다.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # 포즈 주석을 이미지 위에 그립니다.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "        \"\"\"\n",
    "        if results.pose_landmarks != None:\n",
    "            print(type(results.pose_landmarks))\n",
    "            break\n",
    "        \"\"\"\n",
    "\n",
    "        # 보기 편하게 이미지를 좌우 반전합니다.\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e0b696-3e68-4916-9ce7-8e6ca2b147f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4074f14e09674f92a294a459c16f98e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', layout=\"Layout(border='solid')\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m wImg \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mImage( \n\u001b[1;32m      2\u001b[0m     layout \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mLayout(border\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      3\u001b[0m ) \n\u001b[1;32m      4\u001b[0m display(wImg)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcap\u001b[49m\u001b[38;5;241m.\u001b[39misOpened(): \n\u001b[1;32m      7\u001b[0m     ret, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread() \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m ret: \n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# 동영상 파일에서 캡쳐된 이미지를 이미지 파일 스트림으로 다시 인코딩을 한다. \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bae0f71-6a50-4066-acd7-5e7484a5ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stu1/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-5-22 Python-3.8.12 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12045MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 1 tie\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 615.1ms pre-process, 2.9ms inference, 0.6ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.conf = 0.6\n",
    "\n",
    "# Images\n",
    "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8efa4313-4741-4a1c-8196-6c6c3bf923e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m model(\u001b[43mimg\u001b[49m)\n\u001b[1;32m      2\u001b[0m results\u001b[38;5;241m.\u001b[39mprint()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "results = model(img)\n",
    "results.print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f7621-bd9d-4027-90cc-36685f2dc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "wImg = widgets.Image( \n",
    "    layout = widgets.Layout(border=\"solid\") \n",
    ") \n",
    "display(wImg)\n",
    "\n",
    "if cap.isOpened(): \n",
    "    ret, img = cap.read()\n",
    "    while ret:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # 동영상 파일에서 캡쳐된 이미지를 이미지 파일 스트림으로 다시 인코딩을 한다. \n",
    "        results = model(img)\n",
    "        \n",
    "        # xy에는 인식한 각 객체, xy 좌표로 구성된 데이터 프레임 담김\n",
    "        xy = results.pandas().xyxy[0]\n",
    "        \n",
    "        # person에는 인식한 객체중 'person'만 추출\n",
    "        person = xy[xy['name'] == 'person']\n",
    "        \n",
    "        crop_image = []\n",
    "        \n",
    "        # 이미지 \n",
    "        for index, row in person.iterrows():\n",
    "            # 이미지에서 사람에 해당하는 부분만 크롭\n",
    "            # 사진 한장(img) -> 사람 사진 여러장(crop_image[])\n",
    "            crop_image.append(img[int(row['ymin']):int(row['ymax']), int(row['xmin']):int(row['xmax'])])\n",
    "\n",
    "        \n",
    "        for croped in crop_image:\n",
    "            croped = pose(croped)\n",
    "            \n",
    "            # 포즈 결과 출력\n",
    "            tmpStream = cv2.imencode(\".jpeg\", croped)[1].tobytes()\n",
    "            wImg.value = tmpStream \n",
    "\n",
    "        \n",
    "        ret, img = cap.read()\n",
    "else:\n",
    "    print(\"not opened\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887747d-c3f2-4c46-b163-1989bc6bc799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
