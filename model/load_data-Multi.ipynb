{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dc6a8c-a46c-419f-871e-e23f496faf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from untitled import FirebaseMsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c350869-f89f-40ac-80f0-e23846cb3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc # 쿠다 캐시 비우기\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893da140-e1df-47c5-a09b-38b042f7c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local', 'mydb', 'test']\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(host='114.70.235.37', port=27017)\n",
    "print(client.list_database_names())\n",
    "db = client['mydb'].xy_wj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe5fdf7-ac14-4292-a62b-a19d507dab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data= db.find().sort([(\"date\", pymongo.DESCENDING)]).limit(2)\n",
    "data = list(tmp_data)[0]\n",
    "start_date = data[\"date\"] - datetime.timedelta(seconds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897690fd-0a0b-4067-a031-06bf0dce7cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "{'_id': ObjectId('62ce3a8179cf6e56e7301b90'), 'date': datetime.datetime(2022, 7, 13, 12, 22, 41, 190000), 'human_index': 2, 'v0x': 0.0, 'v0y': 0.0, 'v1x': 0.0, 'v1y': 0.0, 'v2x': 0.0, 'v2y': 0.0, 'v3x': 0.0, 'v3y': 0.0, 'v4x': 0.0, 'v4y': 0.0, 'v5x': -540.401123046875, 'v5y': -321.2137451171875, 'v6x': 2.80279541015625, 'v6y': -143.30465698242188, 'v7x': -3.11553955078125, 'v7y': -145.33157348632812, 'v8x': 537.5983276367188, 'v8y': 464.5184020996094, 'v9x': 543.5166625976562, 'v9y': 466.5453186035156}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test = db.find({\"date\": {'$gte' : start_date, '$lt' : data[\"date\"]}}).sort(\"human_index\",pymongo.DESCENDING)\n",
    "k = list(test)[0]\n",
    "print(len(k))\n",
    "print(k)\n",
    "k = k[\"human_index\"]\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b4855c-1bda-4116-917f-c3d41bf955b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k+1):\n",
    "    result= db.find({\"human_index\":i}).sort([(\"date\", pymongo.DESCENDING)]).limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f66d3db-efb7-474e-9c77-ec1110e8bb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('62ce3a8179cf6e56e7301b90'), 'date': datetime.datetime(2022, 7, 13, 12, 22, 41, 190000), 'human_index': 2, 'v0x': 0.0, 'v0y': 0.0, 'v1x': 0.0, 'v1y': 0.0, 'v2x': 0.0, 'v2y': 0.0, 'v3x': 0.0, 'v3y': 0.0, 'v4x': 0.0, 'v4y': 0.0, 'v5x': -540.401123046875, 'v5y': -321.2137451171875, 'v6x': 2.80279541015625, 'v6y': -143.30465698242188, 'v7x': -3.11553955078125, 'v7y': -145.33157348632812, 'v8x': 537.5983276367188, 'v8y': 464.5184020996094, 'v9x': 543.5166625976562, 'v9y': 466.5453186035156}\n"
     ]
    }
   ],
   "source": [
    "print(list(result)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6062931-3d40-4767-89c6-32ae0c68f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input():\n",
    "    \n",
    "    tmp_data= db.find().sort([(\"date\", pymongo.DESCENDING)]).limit(2)\n",
    "    data = list(tmp_data)[0]\n",
    "    start_date = data[\"date\"] - datetime.timedelta(seconds=5)\n",
    "    \n",
    "    max_people = db.find({\"date\": {'$gte' : start_date, '$lt' : data[\"date\"]}}).sort(\"human_index\",pymongo.DESCENDING).limit(2)\n",
    "    k = list(max_people)[0]\n",
    "    k = k[\"human_index\"]\n",
    "    tmp_loader = []\n",
    "    for i in range(k+1):    \n",
    "        result= db.find({\"human_index\":i}).sort([(\"date\", pymongo.DESCENDING)]).limit(2)\n",
    "    \n",
    "        data = list(result)[0]\n",
    "        start_date = data[\"date\"] - datetime.timedelta(seconds=5)\n",
    "        seq = list(db.find({\"date\": {'$gte' : start_date, '$lt' : data[\"date\"]}}))\n",
    "    \n",
    "    \n",
    "        list_data = list()\n",
    "    \n",
    "        for j in seq:\n",
    "            del j['date']\n",
    "            del j['_id']\n",
    "\n",
    "            list_data.append(list(j.values()))\n",
    "        \n",
    "        if(len(list_data) < 30):\n",
    "            return\n",
    "            \n",
    "        index = list(range(5, len(list_data)-1, int(len(list_data)/30)))\n",
    "        list_data = np.array(list_data, dtype=np.float32)\n",
    "    \n",
    "        y_false = np.array([0])\n",
    "        list_data = list_data[index]\n",
    "        list_data = np.array([list_data])\n",
    "    \n",
    "        tmp_data = TensorDataset(torch.from_numpy(list_data), torch.from_numpy(y_false))\n",
    "        tmp_loader.append(DataLoader(tmp_data, shuffle=False, batch_size=batch_size))\n",
    "    \n",
    "    return tmp_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b033cd7-5962-4f6f-944e-b8727bcc2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 32\n",
    "device='cuda'\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_size, output_size, hidden_dim, n_layers):\n",
    "        \n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.lstm = nn.LSTM(seq_size, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        ## lstm_out.shape [batch, seq_len, hidden_dim]\n",
    "        ## hidden [batch, hidden_dim]\n",
    "\n",
    "        # fully connected layer        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        ## out.shape: [n_layer * n_direction, batch, output_size]\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbe1f2a-6b88-4010-afab-9cd7ebfc9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        # nn.init.uniform_(param.data, -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16414ae-a0da-400c-a821-f2ba4a663dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(20, 512, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "n_layers = 4\n",
    "lr = 0.01\n",
    "output_size = 1\n",
    "\n",
    "model = SentimentLSTM(20, output_size, hidden_dim, n_layers)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9e89c5-ab0a-435c-8846-4786c3b236ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_weight.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e1b8cb-a16d-4e87-9f45-3ba6cb6f0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, sequence_length=30):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = 32\n",
    "    acc_list = list()\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        output, h = model(inputs)\n",
    "        \n",
    "        pred = torch.round(output.squeeze())\n",
    "        \n",
    "        accuracy = (pred==labels).sum().item() / 32\n",
    "        acc_list.append(accuracy)\n",
    "\n",
    "        #print('Prediction value, pre-rounding: ', output.item())\n",
    "\n",
    "        # print custom response based on whether test_review is pos/neg\n",
    "    print(acc_list)\n",
    "    print(np.mean(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ef06e-6006-4a9a-9c58-11a00b273a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(model, data_loader_list, sequence_length=30):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = 32\n",
    "    acc_list = list()\n",
    "    \n",
    "    for data_loader in data_loader_list:\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            output, h = model(inputs)\n",
    "\n",
    "            pred = torch.round(output.squeeze())\n",
    "\n",
    "\n",
    "            if int(pred) != 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b01d50-a62e-415b-8ba8-a733c166a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_msg = FirebaseMsg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970975e9-cf3d-4114-b11a-f5682a7147af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, fb_msg):\n",
    "    while True:\n",
    "        data_loader_list = make_input()\n",
    "        if data_loader_list is None:\n",
    "            continue\n",
    "        \n",
    "        if model_run(model, data_loader_list):\n",
    "            print(\"넘어짐 발생\")\n",
    "            fb_msg.falldown()\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a487605-1d30-4f9c-9691-ed4fdfad4a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfb_msg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, fb_msg)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_loader_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m넘어짐 발생\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     fb_msg\u001b[38;5;241m.\u001b[39mfalldown()\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mmodel_run\u001b[0;34m(model, data_loader_list, sequence_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     10\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 12\u001b[0m     output, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(output\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(pred) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mSentimentLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m## lstm_out.shape [batch, seq_len, hidden_dim]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m## hidden [batch, hidden_dim]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# fully connected layer        \u001b[39;00m\n\u001b[1;32m     30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:759\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 759\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    762\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:684\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    680\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    681\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    682\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    683\u001b[0m                        ):\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    686\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    688\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/tools/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20, got 21"
     ]
    }
   ],
   "source": [
    "run(model, fb_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561312d8-6ecb-446d-813c-09d981aa4b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ce15a-27b1-42c9-bc7c-fd74dc2d1c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
